\documentclass{beamer} 
\usepackage[latin1]{inputenc}
\usepackage[english]{babel}
\usetheme{CambridgeUS}

% page formating
\setlength{\parindent}{0pt}

\usepackage{comment}

% fonts
\usefonttheme{professionalfonts}
\usefonttheme[onlymath]{serif}
\usepackage{amsmath, bm}
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx}       % Required for including images
\usepackage{booktabs}       % Top and bottom rules for tables
\usepackage{adjustbox}
% ??
%\usepackage{times}

% code and psendocode
\usepackage{verbatim}
\usepackage{minted}
\usemintedstyle{trac}
\usepackage{algorithm, algorithmic}

% colors
\usepackage{xcolor}
\definecolor{LightGray}{gray}{0.95}
\definecolor{DarkGray}{gray}{0}

% new commands
\newcommand{\real}{\mathbb{R}}
\newcommand{\bernoulli}{\mathrm{Bernoulli}}
\newcommand{\poisson}{\mathrm{Poisson}}
\newcommand{\relu}{\mathrm{relu}}
\newcommand{\sig}{\mathrm{sigmoid}}
\newcommand{\kldiv}{\mathcal{D}_{KL}}
\newcommand{\E}{\mathrm{E}}
\newcommand{\encoder}{\mathrm{encoder}}
\newcommand{\code}[1]{{\small\colorbox{LightGray}{\texttt{#1}}}}

%\setbeamercolor{block title}{use=structure,fg=white,bg=red!75!black}
%\setbeamercolor{block body}{use=structure,fg=black,bg=red!20!white}

% title page
\title{Machine Learning Workshop 2}
\subtitle{Variational Autoencoder}
\author{Jonathan~Guymont}
\date{National Bank of Canada, 2019}

\AtBeginSubsection[]
{
    \begin{frame}<beamer>{Outline}
        \tableofcontents[currentsubsection]
    \end{frame}
}

\begin{document}

\begin{frame}
	\titlepage 
\end{frame}

\begin{frame}{Outline}
	\tableofcontents
\end{frame}

\section{Autoencoders}

\subsection{An example to keep in mind}

\begin{frame}[fragile]{An example to keep in mind}
\begin{figure}
	\centering
	\includegraphics[scale=0.3]{image_to_array}
	\caption{Conversion of a greyscale image to an matrix}
\end{figure}
Note: In the workshop, we will mostly work with linear layers, so we also need to flatten the image.
\begin{minted}[bgcolor=LightGray, fontsize=\footnotesize]{python}
from PIL import Image
def image_to_array(image_path):
    with Image.open(image_path) as img:
        image = img.convert()
        array_image = np.asarray(image, np.float)
    return array_image
\end{minted}
\end{frame}

\subsection{Autoencoders}

\begin{frame}{Autoencoders}
Autoencoers are neural networks that are trained to learn how to map their input to their input. Internally, it has a hidden layer $\bm{h}$ that contains a lossy summary of the relevant features for the task.
\end{frame}
  
\begin{frame}{Autoencoders}
An autoencoder can be seen has a two parts network:
\begin{itemize}
	\item Encoder function: $\bm{z}=f_\phi(\bm{x})$ \pause
	\item Decoder function: $\tilde{\bm{x}} = g_\theta(\bm{z})$ \pause
	\item $\phi$ and $\theta$ are sets of learned parameters
\end{itemize}
\end{frame}

\begin{frame}{Autoencoders}
	The simplest autoencoder is a one layer MLP:
	\Large
	\begin{equation}
	\begin{split}
		\mathbf{z} =& \relu\left(\mathbf{W}_{xz}\mathbf{x}+\mathbf{b}_{xz}\right)~~[\text{encoder}]\\
		\tilde{\mathbf{x}} =& \sig\left(\mathbf{W}_{zx}\mathbf{z}+\mathbf{b}_{zx}\right)~~[\text{decoder}]
	\end{split}
	\end{equation}
\end{frame}

\begin{frame}[fragile]{Pytorch simple autoencoder}
\begin{minted}[bgcolor=LightGray, fontsize=\footnotesize]{python}
class Autoencoder:
	def __init__(self, **kargs):
	    """constructor"""
	    pass
	
	def encoder(self, x):
	    pass
	
	def decoder(self, z)
	    pass
	    
	def forward(self, x):
	    pass
\end{minted}
\end{frame}

\begin{frame}[fragile]{Parameter initialization}
\begin{equation}
\begin{split}
\mathbf{z} =& \relu\left(\mathbf{W}_{xz}\mathbf{x}+\mathbf{b}_{xz}\right)\\
\tilde{\mathbf{x}} =& \sig\left(\mathbf{W}_{zx}\mathbf{z}+\mathbf{b}_{zx}\right) 
\end{split}
\end{equation}
\begin{minted}[bgcolor=LightGray, fontsize=\footnotesize]{python}
class Autoencoder:
    def __init__(self, x_dim, z_dim):
        # encoder parameters \phi
        self.Wxz = xavier_init(size=[x_dim, z_dim])
        self.bxz = Variable(torch.zeros(z_dim), requires_grad=True)
        # decoder parameters \theta
        self.Wzx = xavier_init(size=[z_dim, x_dim])
        self.bzx = Variable(torch.zeros(x_dim), requires_grad=True)
\end{minted}
\end{frame}

\begin{frame}[fragile]{Encoder $f_\phi(x)$}
\begin{align}
	\mathbf{z} =& \relu\left(\mathbf{W}_{xz}\mathbf{x}+\mathbf{b}_{xz}\right)\\
	\bm{\phi}=&\{\mathbf{W}_{xz}, \mathbf{b}_{xz}\} 
\end{align}
\begin{minted}[bgcolor=LightGray, fontsize=\footnotesize]{python}
class Autoencoder:
        ...
    def encoder(self, x):
        z = F.relu(self.Wxz @ z + self.bxz)
        return z
\end{minted}
\end{frame}

\begin{frame}[fragile]{Decoder $g_\theta(x)$}
\begin{align}
	\mathbf{z} 
	=& \sigma\left(\mathbf{W}_{zx}\mathbf{z}+\mathbf{b}_{zx}\right)\\
\bm{\theta}=&\{\mathbf{W}_{zx}, \mathbf{b}_{zx}\} 
\end{align}
\begin{minted}[bgcolor=LightGray, fontsize=\footnotesize]{python}
class Autoencoder:
        ...
    def decoder(self, z):
        x_recon = F.sigmoid(self.Wzx @ z + self.bzx)
        return x_recon
\end{minted}
\end{frame}

\begin{frame}[fragile]{Forward propagation}
\begin{equation}
\begin{split}
\mathbf{z} =& \relu\left(\mathbf{W}_{xz}\mathbf{x}+\mathbf{b}_{xz}\right)\\
\tilde{\mathbf{x}} =& \sig\left(\mathbf{W}_{zx}\mathbf{z}+\mathbf{b}_{zx}\right) 
\end{split}
\end{equation}
\begin{minted}[bgcolor=LightGray, fontsize=\footnotesize]{python}
class Autoencoder:
        ...
    def forward(self, x):
        z = self.encoder(x)
        x_recon = self.decoder(z)
        return x_recon
\end{minted}
\end{frame}

\begin{frame}[fragile]{Pytorch simple autoencoder}
\begin{minted}[bgcolor=LightGray, fontsize=\tiny]{python}
class Autoencoder:
    def __init__(self, x_dim, z_dim):
        # encoder parameters
        Wxz = xavier_init(size=[x_dim, z_dim])
        bxz = Variable(torch.zeros(z_dim), requires_grad=True)
        # decoder parameters
        Wzx = xavier_init(size=[h_dim, x_dim])
        bzx = Variable(torch.zeros(X_dim), requires_grad=True)
    
    def encoder(self, x):
        z = F.relu(self.Wxh @ x + self.bxh)
        return z
    
    def decoder(self, z):
        x_recon = F.sigmoid(self.Wzx @ z + self.bzx)
        return x_recon
    
    def forward(self, x):
        z = self.encoder(x)
        x_recon = self.decoder(z)
        return x_recon
\end{minted}
\end{frame}

\begin{frame}{Autoencoder - Loss Function}
If you treat the problem like a \textit{regression}\footnote{If you do regression, you don't have to apply sigmoid in the decoder.}, use the mean square error between the inputs and the reconstructions
\begin{equation}
\mathcal{L} = \sum_{i=1}^d (x_i-\tilde{x}_i)^2
\end{equation}
\end{frame}

\begin{frame}{Training Autoencoders}
\begin{algorithm}[H]
	\begin{algorithmic}
		\REQUIRE Learning rate $\eta$
		\REQUIRE Initial parameter $\bm{\omega}_0$
		\REQUIRE Number of epochs $T$
		\FOR{$i=1$ to $T$}
		\STATE $X =X^{train}$.copy() and $Y =Y^{train}$.copy()
		\WHILE{$X$ is not empty}
		\STATE Sample $\{x^{(1)},...,x^{(m)}\}$ from $X$ and $\{y^{(1)},...,y^{(m)}\}$ from $Y$
		\STATE Remove samples from $X$ and $Y$
		\STATE Compute gradient $\bm{g}_t=\frac{1}{m}\nabla_{\bm{\omega}}\sum_i \mathcal{L}(\tilde{x}^{(i)}, x^{(i)})$  
		\STATE Apply update: $\bm{\omega}_t=\bm{\omega}_{t-1}-\eta\cdot \bm{g}_t$
		\ENDWHILE
		\ENDFOR
	\end{algorithmic}
	\caption{Pseudocode for Stochastic Gradient Training}
	\label{alg:sgd}
\end{algorithm}
\end{frame}

\begin{frame}[fragile]{Pytorch Stochastic Gradient}
\begin{minted}[bgcolor=LightGray, fontsize=\tiny]{python}
def train(self, trainloader, num_epochs, learning_rate):

    for epoch in range(num_epochs):
        for inputs, targets in trainloader:
            batch_size = inputs.size(0)
            x_tilde = self.forward(x)

            loss = F.mse_loss(x, x_tilde)

            # Use autograd to compute the derivative of the loss w.r.t 
            # all Tensors with requires_grad=True. After calling `loss.backward()`, 
            # conv_weight.grad, dense_weight.grad, and dense_bias.grad 
            # will be Tensors equal to the gradient of the loss with respect 
            # to the filters of the cnn layer, the weight of the fully connected layer, and 
            # the bias of the fully connected layer respectively.
            loss.backward()
            
            # Apply gradient descent to all the leaned parameters
            # The derivative of the loss is giving us the direction
            # where the funtion increase. Thus we go in the 
            # opposite direction. Using torch.no_grad() tells pytorch
            # to not include thes operation in the computational graph.
            # Instead, gradient descent is goning to be applied `inplace`.
            with torch.no_grad():
                self.W_xz -= learning_rate * self.W_xz.grad
                self.b_xz -= learning_rate * self.b_xz.grad
                self.W_zx -= learning_rate * self.W_zx.grad
                self.b_zx -= learning_rate * self.b_zx.grad
\end{minted}
\end{frame}

\begin{frame}{Autoencoders}

To summarize
\begin{itemize}
	\item A neural network encodes $x$ in a hidden state $z$ of smaller dimension.
	\item Another neural network decodes $z$ to reconstruct $x$.
	\item A sound loss function could be the mean square error between the inputs and their reconstructions.
	\item Both network can be train at the same time with stochastic subgradient method.
\end{itemize}
\end{frame}

\section{Variational autoencoders}

\subsection{Generative Models}

\begin{frame}{Generative Models}
Represent the probability distribution of either $P(X,Y)$ or $P(X)$. In the case of \textit{density estimation}, we are looking for a representation of

\huge
\[
	x \sim P_\theta(X)
\]
\normalsize
For example, $x\sim \mathcal{N}(x;\bm{\mu}_{\text{mle}}, \bm{\sigma}_{\text{mle}})$ \\

Problem: Most parametric distributions make strong (and often wrong) assumptions about the distribution.
\end{frame}

\subsection{Latent variable models}

\begin{frame}{Latent variable models}
	We can model the distribution of $x$ as a function of a latent variable $z$
	\[
	p(x) = \int p_\theta (x|z) p(z) dz
	\]
	where the distribution of $z$ is chosen. A typical choice for $z$ is 
	\[
		z\sim \mathcal{N}(z;\bm{0}, \bm{I})
	\]
	Then we can train a model to learn a good representation of $p_\theta (x|z)$ with stochastic gradient descent.
\end{frame}

\begin{frame}{Latent variable models}
	\[
		p(x) = \int p_\theta (x|z) p(z) dz
	\]
	Once we have a good representation of $p_\theta (x|z)$, we can sample from $p(x)$ by first sampling
	\[
		z'\sim p(z)
	\]
	and then sampling\\
	\[
		x'\sim p(x|z')
	\]
\end{frame}

\begin{frame}{Latent variable models}
	Problem 1: To learn $p_\theta(x|z)$ using stochastic gradient descent, we need to know a good mapping 
	\[
		f: \mathcal{Z}\times \Theta \mapsto \mathcal{X}
	\]
	In other words when we sample $x\sim p(x|z')$ we need to know which $x$ is likely to be generated by this particular $z'$ in order to train the model.
\end{frame}

\begin{frame}{Latent variable models}
	Solution: the prior of the latent space can be written has
	\[
	p(z) = \int p(z|x) p(x) dx
	\]
	During training, We can sample $z$ by sampling \\
	\[
	x'\sim p(x)
	\]
	and then 
	\[z\sim p(z|x')\]
	
	 The training set comes from $p(x)$ so we can sample from it. This will reduce the space of the latent variable a lot and allow the model to learn efficiently.
\end{frame}

\begin{frame}{Latent variable models}
	Problem 2: $p(z|x)$ is intractable. \\
	
	Solution: use an approximation $q_\phi(z|x)$\\
	
	To summarize
	\begin{itemize}
		\item Sample $x \sim D_n$
		\item Sample $z \sim q_\phi(z|x)$
		\item Sample $\tilde{x} \sim p_\theta(x|z)$
	\end{itemize}
	The sets of parameters to learn are $\phi$ and $\theta$ and they should be learn such that the marginal likelihood $p(x)$ is maximized.
\end{frame}

\begin{frame}{Latent variable models}
	Before looking at how we can train this model efficiently, let's take a closer look at how it works.
\end{frame}

\begin{frame}[fragile]{Probabilistic Encoder $q_\phi(z|x)$}
	Example: Gaussian MLP as encoder
	\begin{itemize}
		\item $\bm{\mu}_z,~\log\bm{\sigma}_z^2=f(\mathbf{x};\bm{\phi})$
		\begin{itemize}
			\item $\mathbf{h} = \relu\left(\mathbf{W}_{xh}\mathbf{x}+\mathbf{b}_{xh}\right)$
			\item $\bm{\mu} = \mathbf{W}^{(1)}_{hz}\mathbf{h}+\mathbf{b}^{(1)}_{hz}$
			\item $\log \bm{\sigma}^2 = \mathbf{W}^{(2)}_{hz}\mathbf{h}+\mathbf{b}^{(2)}_{hz}$
		\end{itemize}
		\item $q_\phi(\mathbf{z}|\mathbf{x})=\mathcal{N}(\mathbf{z};\bm{\mu}_z, \bm{\sigma}_z^2)$
		\item $\phi = \{\mathbf{W}^{(1)}_{hz}, \mathbf{b}^{(1)}_{hz}, \mathbf{W}^{(2)}_{hz}, \mathbf{b}^{(2)}_{hz}, \mathbf{W}_{xh}, \mathbf{b}_{xh}\}$
	\end{itemize}
\begin{minted}[bgcolor=LightGray, fontsize=\footnotesize]{python}
class VAE:
    ...
    def encoder(self, x):
        # Encoder network. Return the parameter of q(z|x)
        h = relu(self.Wxh @ x + self.bxh)
        mu = self.Whz_mu @ h + self.bhz_mu      
        logvar = self.Whz_var @ h + self.bhz_var
        reture mu, logvar
\end{minted}
\end{frame}

\begin{frame}[fragile]{Sampling $z\sim q_\phi(z|x)$}
Example: Sampling $\mathbf{z}$ from $q_\phi(\mathbf{z}|\mathbf{x})$
\begin{itemize}
	\item $\mathbf{z}\sim \mathcal{N}(\bm{\mu}_z, \bm{\sigma}_z)$
	\begin{itemize}
		\item $\bm{\mu}_z,~\log\bm{\sigma}^2_z=f(\mathbf{x};\bm{\phi})$
		\item $\bm{\epsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$
		\item $\mathbf{z} = \bm{\mu}_z + \bm{\sigma}_z \odot \bm{\epsilon} $
	\end{itemize}
\end{itemize}
\begin{minted}[bgcolor=LightGray, fontsize=\footnotesize]{python}
class VAE:
    ...
    def _sample_z(self, mu, logvar):
        epsilon = Variable(torch.randn(mu.size())
        sigma = torch.exp(logvar / 2)
        return mu + sigma * epsilon
\end{minted}
\end{frame}

\begin{frame}[fragile]{Probabilistic Decoder $p_\theta(x|z)$}
	Example: Gaussian MLP as decoder
	\begin{itemize}
		\item $\bm{\mu}_x,~\log\bm{\sigma}_x^2=g(\mathbf{x};\bm{\theta})$
		\begin{itemize}
			\item $\mathbf{h} = \relu\left(\mathbf{W}_{zh}\mathbf{x}+\mathbf{b}_{zh}\right)$
			\item $\bm{\mu}_x = \mathbf{W}^{(1)}_{hz}\mathbf{h}+\mathbf{b}^{(1)}_{hz}$
			\item $\log \bm{\sigma}_x^2 = \mathbf{W}^{(2)}_{hz}\mathbf{h}+\mathbf{b}^{(2)}_{hz}$
		\end{itemize}
		\item $p_{\bm{\theta}}(\mathbf{x}|\mathbf{z})=\mathcal{N}(\mathbf{x};\bm{\mu}_x, \log\bm{\sigma}_x^2)$
		\item $\bm{\theta} = \{\mathbf{W}_{zh}, \mathbf{b}_{zh}, \mathbf{W}_{zx}, \mathbf{b}_{zx}\}$
	\end{itemize}
\begin{minted}[bgcolor=LightGray, fontsize=\footnotesize]{python}
class VAE:
    ...
    def decoder(self, z):
        # Decoder network. Reconstruct the input from 
        # the latent variable z
        h = relu(self.Wzh @ z + self.bzh)
        mu_x = self.Whx_mu @ h + self.bhx_mu
        logvar_x = self.Whx_var @ h + self.bhx_var
        return mu_x, logvar_x
\end{minted}
\end{frame}

\begin{frame}{Latent variable models}
	To summarize:
	\begin{itemize}
		\item Sample $x \sim D_n$
		\item Sample $z \sim q_\phi(z|x)$
		\item Sample $\tilde{x} \sim p_\theta(x|z)$
	\end{itemize}
	The sets of parameters to learn are $\phi$ and $\theta$ and they should be learn such that the marginal likelihood $p(x)$ is maximized.
\end{frame}

\subsection{Variational Lower Bound}

\begin{frame}{Training VAE}
	We need $p_\theta(x|z)$ to be such that the marginal likelihood $p(x)$ is maximized. \\
	
	In other words, the lost function should be
	\huge
	\[
		\mathcal{L}(x;\phi, \theta) = -\log p(x)	
	\]
\end{frame}

\begin{frame}{Kullback-Leibler divergence}

	\begin{block}{Kullback-Leibler divergence}
		Let $p(x)$ and $q(x)$ be probability measures over a set $X$, and $q(x)$ is absolutely continuous with respect to $p(x)$, then the Kullback-Leibler divergence from $p$ to $q$ is defined as
		\begin{equation*}
			\begin{split}
			\kldiv[q(x)||p(x)] 
			=& \E_{x\sim q(x)}[ \log q(x) - \log p(x)]\\
			=& \int q(x)(\log q(x)-\log p(x))dx\\
			=& \int q(x)\log q(x)dx-\int q(x)\log p(x)dx
			\end{split}
		\end{equation*}
	\end{block}
	\begin{block}{Gibbs' inequality}
		\begin{equation*}
			\kldiv[q(x)||p(x)] \geq 0
		\end{equation*} 
	\end{block}
\end{frame}

\begin{frame}{Variational Lower Bound}
	\begin{corollary}
		The marginal log-likelihood $\log p(x)$ is lower bounded by the \textit{variational lower bound} i.e.
		\[
			\log p(x)  \geq \E_{z\sim q(z|x)}\log p(x|z)- \kldiv[q(z|x)||p(z)]
		\]
	\end{corollary}
	Hence, our loss function is
	\Large
	\[
		\mathcal{L}(x;\phi, \theta) = \E_{z\sim q(z|x)}\log p(x|z)- \kldiv[q(z|x)||p(z)]
	\] 
\end{frame} 

\begin{frame}{Proof of the Variational Lower Bound}
	\begin{proof}
		\scriptsize
		First, let's compute the Kullback-Leibler divergence $\kldiv$ between $p(z|x)$ and $q(z|x)$
		\begin{equation*}
		\begin{split}
		\kldiv[q(z|x)||p(z|x)] 
		=& \E_{z\sim q(z|x)}[ \log q(z|x) - \log p(z|x)]\\
		=& \E_{z\sim q(z|x)}[ \log q(z|x) - \log p(x|z) - \log p(z) + \log p(x)]\\
		=& \log p(x) + \E_{z\sim q(z|x)}[ \log q(z|x)  - \log p(z)] - \E_{z\sim q(z|x)}\log p(x|z)\\
		=& \log p(x) - \kldiv[q(z|x)||p(z|x)] - \E_{z\sim q(z|x)}\log p(x|z)\\
		\end{split}
		\end{equation*}
		Sending all the terms on the right except $\log p(x)$ gives
		\[
			\log p(x) = \E_{z\sim q(z|x)}\log p(x|z)- \kldiv[q(z|x)||p(z)] + \kldiv[q(z|x)||\log p(z|x)]
		\]
		Finally, because of Gibbs' inequality, we have
		\[
			\boxed{\log p(x)  \geq \E_{z\sim q(z|x)}\log p(x|z)- \kldiv[q(z|x)||p(z)]}
		\]
	\end{proof}
\end{frame}

\begin{frame}[fragile]{Solution of $\kldiv[q(z|x)||p(z)]$}
	\begin{block}{Lemma}
		Let $z\in \real^J$ be a standard multivariate Gaussian distribution and let $z|x$ be a multivariate Gaussian distribution with mean $\bm{\mu}_z$ and standard deviation $\bm{\sigma}_z$. Then the KL-divergence from $p(z)$ to $q_\phi(z|x)$ is 
		\begin{equation*}
		\kldiv[q(z|x)||p(z)] 
		= \frac{1}{2}\sum_{j=1}^J \mu_j^2 + \sigma_j^2 - 1 - \log \sigma_j^2
		\end{equation*}
	\end{block}
\begin{minted}[bgcolor=LightGray, fontsize=\footnotesize]{python}
def kl_divergence(mu, log_sigma):
    sigma = torch.exp(log_sigma)
    return .5 * torch.sum(mu**2 + sigma**2 - 1 - 2*log_sigma, axis=1)
\end{minted}
\end{frame}

\begin{frame}{Proof for the Solution of $\kldiv[q(z|x)||p(z)]$}
\begin{proof}
	\tiny
	According to the definition of the KL-divergence we have
	
	\vspace{-5pt}
	
	\begin{equation*}
	\kldiv[q_\phi(z|x)||p(z)] 
	= \int q_\phi(z|x)\log q_\phi(z|x)dx-\int q_\phi(z|x)\log p(z))dx
	\end{equation*}
	
	\vspace{-5pt}
	
	Solving the first integral gives
	
	\vspace{-5pt}
	
	\begin{equation}
		\begin{split}
			\int q(z|x)\log q(z|x)dz 
			=& \int \mathcal{N}(z;\mu, \sigma)\log \mathcal{N}(z;\mu, \sigma)\\
			=& -\frac{J}{2} \log 2\pi - \frac{1}{2}\sum_{j=1}^J (1+\log \sigma_j^2)
		\end{split}
		\label{eq:klgauss1}
	\end{equation}
	
	\vspace{-5pt}
	
	Now solving the second integral
	
	\vspace{-5pt}
	
	\begin{equation}
		\begin{split}
			\int q(z|x)\log p(z)dz 
			=& \int \mathcal{N}(z;\mu, \sigma)\log \mathcal{N}(z;0, I)\\
			=& -\frac{J}{2} \log 2\pi - \frac{1}{2}\sum_{j=1}^J (\mu_j^2+\sigma_j^2)
		\end{split}
	\label{eq:klgauss2}
	\end{equation}
	
	\vspace{-5pt}
	
	where $J$ is the dimension of $z$. Finally, subtracting (\ref{eq:klgauss2}) from (\ref{eq:klgauss1}) gives  
	
	\vspace{-5pt}
	
	\begin{equation*}
		\kldiv[q(z|x)||p(z)] 
		= \frac{1}{2}\sum_{j=1}^J \mu_j^2 + \sigma_j^2 - 1 - \log \sigma_j^2
	\end{equation*}
\end{proof}
\end{frame}

\begin{frame}[fragile]{VAE Loss Function}
	The loss function for a standard Gaussian latent variable and a multivariate Gaussian posterior distribution on $x$ is given by 
	\begin{equation}
	\begin{split}
			\mathcal{L}(x;\phi, \theta) 
			=& -\E_{z\sim q(z|x)}\log p(x|z) + \kldiv[q(z|x)||p(z)]\\
			=& -\log \mathcal{N}(x;\bm{\mu}_x, \bm{\sigma}_x) + \frac{1}{2}\sum_{j=1}^J \mu_j^2 + \sigma_j^2 - 1 - \log \sigma_j^2
	\end{split}
	\end{equation}
\end{frame}

\begin{frame}{Choice of $p_\theta(x|z)$}
	In our example, we used $p_\theta(x|z)=\mathcal{N}(x;\bm{\mu}_z, \bm{\sigma}_z)$. In practice you can use any distribution you want. \\
	
	Some important aspects to consider:
	\begin{itemize}
		\item The image of the distribution must correspond to the input space.\pause
		\item Choose a distribution that makes sense based on your knowledge of the problem. \pause
		\item If the domain is continuous on the real number, a normal distribution can make sense. \pause
		\item If the domain is discrete, choose a discrete distribution like a Bernoulli or a Poisson.
	\end{itemize}
\end{frame}

\section{Experiment}

\subsection{Unsupervised Spam Detection}

\begin{frame}{Unsupervised Spam Detection}
\begin{itemize}
	\item SMS Spam Collecion Data Set\footnote{https://archive.ics.uci.edu/ml/datasets/sms+spam+collection}\begin{itemize}
		\item Number of instances: 5574
		\item Number of spams: 747 ($\sim 13 \%$)
	\end{itemize} 
	\item Spam detection
	\begin{itemize}
		\item Estimate the distribution of all the text messages
		\item Evaluate the density of each text message
		\item Text messages with low density are classified as spam
	\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Data}
\begin{itemize}
	\item Examples of \textbf{non spams}
	\begin{itemize}
		\item \textit{What are you doing? how are you?}
		\item \textit{Ok lar... Joking wif u oni...}
	\end{itemize} 
	\item Examples of \textbf{spams}
	\begin{itemize}
		\item \textit{Your mobile No 07808726822 was awarded a L2,000 Bonus Caller Prize on 02/09/03! This is our 2nd attempt to contact you! Call 0871-872-9758 as soon as possible.}
	\end{itemize}
\end{itemize}
\end{frame}

%\begin{frame}{Anomaly detection and autoencoder}
%	Some anomaly detection methods:
%	\begin{itemize}
%		\item Statistical: A data point is defined as an anomaly if the density of it being generated from the model is below a threshold
%		\item Proximity based: A data point is defined as an anomaly if it is \textit{islolated} (e.g. far from clusters centroid)
%		\item Deviation based: Use reconstruction error to detect anomaly (e.g. $k$-most significant principle component (PCA) and autoencoders based methods)
%	\end{itemize}
%\end{frame}

\subsection{Preprocessing}

\begin{comment}
\begin{frame}{Preprocessing}
	\begin{itemize}
		\item Some terminology
		\begin{itemize}
			\item Vocabulary: Set of all the word types in the training set $\{w_1,...,w_{|\text{Vocabulary}|}\}$.
			\item Training corpus: Set of all text messages in the training set $\{x^{(1)},...,x^{(n)}\}$.
			\item Bag of words: Vector representation of a sentence.
		\end{itemize}
	\end{itemize}
\end{frame}
\end{comment}

\begin{frame}[fragile]{Bag of words}
	\begin{itemize}
		\item $x^{(i)}=\textit{What you doing?how are you?}$ (original sentence)
		\item $x^{(i)}=\textit{what you do how be you}$ (after preprocessing)
		\item $x^{(i)}=[0\cdots0~1~0\cdots~0~2~0\cdots0]$ (vector representation)
	\end{itemize}
\begin{minted}[bgcolor=LightGray, fontsize=\footnotesize]{python}
x = 'What you doing?how are you?'

x = preprocess(x)
print(x)
>>> 'what you do how be you'

x = vectorize(x)
print(x)
>>> [0 0 0 ... 0 0 0]
\end{minted}
\end{frame}

\begin{frame}[fragile]{Bag of words}
\begin{itemize}
	\item Bag of words
	\begin{itemize}
		\item All examples in the corpus have the same length. 
		\item The value of a position in the vector is equal to the frequency of the corresponding word in the example.
		\item $x^{(i)}=[0\cdots0~1~0\cdots~0~2~0\cdots0]$
	\end{itemize}
	\item Binary bag of words
	\begin{itemize}
		\item All examples in the corpus have the same length. 
		\item Each element in the vector is either 0 or 1.
		\item $x^{(i)}=[0\cdots0~1~0\cdots~0~1~0\cdots0]$
	\end{itemize}
\end{itemize}
\end{frame}

\begin{comment}
\subsection{Binary Cross Entropy}

\begin{frame}{Binary Cross Entropy}
	Recall that the density of a Bernoulli distribution is given by 
	\begin{equation}
	\bernoulli(x;\gamma) = \gamma^x(1-\gamma)^{1-x}
	\end{equation}
	where $\gamma\equiv p(x=1)$.
\end{frame}

\begin{frame}{Binary Cross Entropy}
	Thus the log-likelihood of a Bernoulli distribution is
	\begin{equation}
	\log p(x) = x\log\gamma + (1-x)\log(1-\gamma)
	\end{equation}
\end{frame}

\begin{frame}[fragile]{Binary Cross Entropy}
	If your have binary features, i.e. $x_i\in \{0,1\}$ for $i=1,...,d$, then the output of the decoder can be interpreted has the parameter of a Bernoulli. Thus the likelihood of the input $x$ can be compute has 
	\begin{equation}
	p(x|z)=\prod_{i=1}^dp(x_i|z) = \prod_{i=1}^d\gamma_i^{x_i}(1-\gamma_i)^{1-x_i}
	\end{equation}
	Note that this works only if $\gamma_i\in (0, 1)$. To ensure it, apply sigmoid element wise on the output of the decoder.
\end{frame}

\begin{frame}[fragile]{Binary Cross Entropy}
And the log-likelihood is given by
	\begin{equation*}
	\log p(x|z) = -\sum_{i=1}^d x_i\log \gamma_i + (1-x_i)\log(1-\gamma_i) 
	\end{equation*}
\begin{minted}[bgcolor=LightGray, fontsize=\footnotesize]{python}
# in pytorch
functional.binary_cross_entropy(param, x)
\end{minted}
\end{frame}
\end{comment}

\subsection{Decoders}

\begin{frame}[fragile]{Bernoulli Decoder $p_\theta(x|z)$}
\begin{itemize}
	\item $\bm{\gamma}=g(z;\bm{\theta})$
	\begin{itemize}
		\item $\mathbf{h} = \relu\left(\mathbf{W}_{zh}\mathbf{z}+\mathbf{b}_{zh}\right)$
		\item $\bm{\gamma} = \sigma\left(\mathbf{W}_{hx}\mathbf{h}+\mathbf{b}_{hx}\right)$
	\end{itemize}
	\item $p_\theta(\mathbf{x}|\mathbf{z})=\prod_i\gamma_i^{x_i}(1-\gamma_i)^{(1-x_i)}$
\end{itemize}
\begin{minted}[bgcolor=LightGray, fontsize=\footnotesize]{python}
class VAE:
    ...
    def decoder(self, z):
        h = relu(self.W_zh @ z + self.b_zh)  
        gamma = F.sigmoid(self.W_hx @ h + self.b_hx)      
        reture gamma
\end{minted}
\end{frame}

\begin{frame}[fragile]{Poisson Decoder $p_\theta(x|z)$}
\begin{itemize}
	\item $\bm{\lambda}=g(\mathbf{z};\bm{\theta})$
	\begin{itemize}
		\item $\mathbf{h} = \relu\left(\mathbf{W}_{zh}\mathbf{z}+\mathbf{b}_{zh}\right)$
		\item $\log\bm{\lambda} = \tanh\left(\mathbf{W}_{hx}\mathbf{h}+\mathbf{b}_{hx}\right)$
	\end{itemize}
	\item $p_\theta(\mathbf{x}|\mathbf{z})=\prod_i e^{-\lambda_i}\frac{\lambda_i^{x_i}}{x_i!}$
\end{itemize}
\begin{minted}[bgcolor=LightGray, fontsize=\footnotesize]{python}
class VAE:
    ...
    def decoder(self, z):
        h = relu(self.W_zh @ z + self.b_zh  
        loglambda = F.tanh(self.W_hx @ h + self.b_hx)
        reture loglambda
\end{minted}
\end{frame}

\subsection{Spam Detector}

\begin{frame}{Anomaly detection}  
\begin{algorithm}[H]
	\begin{algorithmic}
		\REQUIRE Density Threshold $\mathcal{T}$
		\REQUIRE Sets of trained parameters $\phi$ and $\theta$
		\STATE Sample $\mathbf{z}^{(1)},...,\mathbf{z}^{(1)}$ from $q_\phi(\mathbf{z}|\mathbf{x}^{(i)})$
		\STATE Compute $p(\mathbf{x})\approx\frac{1}{L}\sum_{i=1}^L p_\theta(\mathbf{x}|\mathbf{z}^{(i)})$
		\STATE If $p(\mathbf{x})<\mathcal{T}$, then $\mathbf{x}$ is classified as an outlier
	\end{algorithmic}
	\caption{Pseudocode for Anomaly detection}
	\label{alg:se}
\end{algorithm}
\end{frame}

\begin{frame}{Anomaly detection}  
\begin{algorithm}[H]
	\begin{algorithmic}
		\REQUIRE Percentile threshold $k$
		\REQUIRE Trained parameters $\phi$ and $\theta$
		\FOR{$i=1$ to $N$}
		\STATE Sample $\mathbf{z}^{(1)},...,\mathbf{z}^{(1)}$ from $q_\phi(\mathbf{z}|\mathbf{x}^{(i)})$
		\STATE Compute $p(\mathbf{x}^{(i)})\approx\frac{1}{L}\sum_{i=1}^L p_\theta(\mathbf{x}|\mathbf{z}^{(i)})$
		\ENDFOR
		\STATE $\mathcal{T}=p(\bm{x}^{(\tau)})$ where $|\{\bm{x} : p(\bm{x}) < p(\bm{x}^{(\tau)})\}|=k\cdot N$
	\end{algorithmic}
	\caption{Pseudocode to find the density threshold}
	\label{alg:se}
\end{algorithm}
\end{frame}

\begin{frame}{Results}
\begin{figure}[h]
	\centering
	\begin{tabular}{cccc}
		\includegraphics[width=0.4\textwidth]{kldiv_03.png}
		& 
		\includegraphics[width=0.4\textwidth]{logp_03.png}\\
		\includegraphics[width=0.4\textwidth]{precisions_03.png}
		& 
		\includegraphics[width=0.4\textwidth]{recall_03.png}\\
	\end{tabular}
\end{figure}
\end{frame}


\begin{frame}{Results}
\begin{table}[h]
	\caption{Test results of the spam detection and the modelization of the text distribution. The test data represent 50\% of the entire data set. The confidence interval are due to the randomness caused by the sampling of the latent variable $\mathbf{z}$ when evaluating the probability.}
	\centering
	\begin{adjustbox}{max width=\textwidth}
		\begin{tabular}{lrrrr}
			\toprule
			model & precision &  recall & $\log p(\mathbf{x}|\mathbf{z})$ & $\kldiv$\\
			\midrule
			BOW &  $0.62\pm 0.006$ &$0.16\pm 0.004$ & $-258.62\pm 0.33$ & $8.27\pm 2.580$\\
			Binary BOW & $0.64\pm 0.003$ & $0.22\pm 0.003$ & $-48.89\pm 0.03$ & $1.25\pm 0.000$\\
			BOC & $0.79\pm0.005$ & $0.26\pm 0.004$ & $-279.95\pm 0.41$ & $14.17\pm 4.560$ \\
			\bottomrule
		\end{tabular}
	\end{adjustbox}
	\label{table:result}
\end{table}
\end{frame}


\end{document}