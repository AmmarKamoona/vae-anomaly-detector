\documentclass{beamer} 

\usetheme{CambridgeUS}
\usefonttheme{professionalfonts}
\usefonttheme[onlymath]{serif}
\usepackage[latin1]{inputenc}
\usepackage{times}
\usepackage{amsmath}
\usepackage{verbatim}
\usepackage{bm}

% new command
\newcommand{\real}{\mathbb{R}}

\title{Machine Learning Workshop 2}
\subtitle{Variational Autoencoder}
\author{Jonathan~Guymont}
\date{National Bank of Canada, 2018}

\AtBeginSubsection[]{
    \begin{frame}<beamer>{Outline}
        \tableofcontents[currentsection,currentsubsection]
    \end{frame}
}

\begin{document}

\section{Autoencoders}

\begin{frame}{Autoencoders}
Autoencoers are neural network that are trained to learn how to map their input to their input. Internally, it has an hidden layer $\bm{h}$ that contains a lossy summary of the relevant feature for the task.\\

An autoencoder can be seen has a two parts network
\begin{itemize}
	\item Encoder function: $\bm{h}=f(\bm{x})$
	\item Decoder function: $\tilde{\bm{x}} = g(\bm{h})$
\end{itemize}

The simplest autoencoder is a MLP:
\begin{equation}
\begin{split}
\bm{h} =& \sigma_1\left(W_{xh}x\right)\\
\tilde{x} =& \sigma_2\left(W_{hx}\bm{h}\right) 
\end{split}
\end{equation}
\end{frame}
 
\section{Generative Model}

\section{Variational autoencoder}
\begin{frame}{Autoencoders}

\begin{itemize}
	\item Encoder function: $\bm{h}=f(\bm{x})$
	\item Decoder function: $\tilde{\bm{x}} = g(\bm{h})$
\end{itemize}

The simplest autoencoder is a MLP:
\begin{equation}
\begin{split}
\bm{h} =& \sigma_1\left(W_{xh}x\right)\\
\tilde{x} =& \sigma_2\left(W_{hx}\bm{h}\right) 
\end{split}
\end{equation}
\end{frame}

\section{RNN}

\section{Attention Mechanism}

\section{DRAW}




\end{document}