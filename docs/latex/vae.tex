\documentclass{beamer} 
\usepackage[latin1]{inputenc}
\usetheme{CambridgeUS}

% page formating
\setlength{\parindent}{0pt}

% fonts
\usefonttheme{professionalfonts}
\usefonttheme[onlymath]{serif}
\usepackage{amsmath, bm}

% ??
%\usepackage{times}

% code and psendocode
\usepackage{verbatim}
\usepackage{minted}
\usemintedstyle{trac}
\usepackage{algorithm, algorithmic}

% colors
\usepackage{xcolor}
\definecolor{LightGray}{gray}{0.95}
\definecolor{DarkGray}{gray}{0}

% new commands
\newcommand{\real}{\mathbb{R}}
\newcommand{\code}[1]{{\small\colorbox{LightGray}{\texttt{#1}}}}

% title page
\title{Machine Learning Workshop 2}
\subtitle{Variational Autoencoder}
\author{Jonathan~Guymont}
\date{National Bank of Canada, 2018}

\AtBeginSubsection[]{
    \begin{frame}<beamer>{Outline}
        \tableofcontents[currentsection,currentsubsection]
    \end{frame}
}

\begin{document}

\begin{frame}
	\titlepage 
\end{frame}

\begin{frame}{Outline}
	\tableofcontents
\end{frame}

\section{Autoencoders}

\begin{frame}{Autoencoders}
Autoencoers are neural network that are trained to learn how to map their input to their input. Internally, it has an hidden layer $\bm{h}$ that contains a lossy summary of the relevant feature for the task.
\end{frame}
 
\begin{frame}{Autoencoders}
An autoencoder can be seen has a two parts network
\begin{itemize}
	\item Encoder function: $\bm{h}=f(\bm{x})$
	\item Decoder function: $\tilde{\bm{x}} = g(\bm{h})$
\end{itemize}
\end{frame}

\begin{frame}{Autoencoders}
The simplest autoencoder is a one layer MLP:
\begin{equation}
\begin{split}
\bm{h} =& \sigma_1\left(W_{xh}x\right)\\
\tilde{x} =& \sigma_2\left(W_{hx}\bm{h}\right) 
\end{split}
\end{equation}
\end{frame}

\begin{frame}[fragile]{Pytorch autoencoder}
\begin{minted}[bgcolor=LightGray, fontsize=\footnotesize]{python}
import numpy
def forward():
# fjdksjfksjfkls
\end{minted}
\end{frame}

\section{Generative Model}


\section{Variational autoencoder}

\section{RNN}

\section{Attention Mechanism}

\section{DRAW}

\begin{frame}{Anomaly detection}  
\begin{algorithm}[H]
	\begin{algorithmic}
		\REQUIRE Learning rate $\epsilon_k$
		\REQUIRE Initial parameter $\bm{w}_0$
		\REQUIRE Number of epochs $T$
		\FOR{$i=1$ to $T$}
		\STATE Compute gradient $\bm{g}_t=\frac{1}{m}\nabla_w\sum_i L(h_{w_{t-1}}(\bm{x}^{(i)}), \bm{y}^{(i)})$ 
		\STATE Apply update: $\bm{w}_t=\bm{w}_{t-1}-\epsilon \bm{g}_t$
		\ENDFOR
	\end{algorithmic}
	\caption{Pseudocode for Batch Gradient Descent}
	\label{alg:seq}
\end{algorithm}
\end{frame}

\begin{frame}[fragile]{Anomaly detection}
\begin{minted}[bgcolor=LightGray, fontsize=\footnotesize]{python}
import numpy
def forward():
    # fjdksjfksjfkls
\end{minted}
\end{frame}




\end{document}