{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import math\n",
    "from itertools import chain\n",
    "import pandas\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = '../data/spam.csv'\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    \"\"\"\n",
    "    Load spam data into a pandas dataframe\n",
    "    \"\"\"\n",
    "    dataframe = pandas.read_table(path, delimiter=',', encoding='latin-1')\n",
    "    return dataframe[['v1', 'v2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     v1                                                 v2\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
      "You are a winner U have been specially selected 2 receive å£1000 or a 4* holiday (flights inc) speak to a live operator 2 claim 0871277810910p/min (18+) \n",
      "you are a winner u have been specially selected 2 receive å\ts\tdollarsign\t\ts\tnumber\t or a 4\t holiday \tflights inc\t speak to a live operator 2 claim \t\tlongdigit\t\tp\tmin \t\tnumber\t\t\t \n",
      "['you', 'are', 'a', 'winner', 'u', 'have', 'been', 'specially', 'selected', '2', 'receive', 'å', 's', 'dollarsign', 's', 'number', 'or', 'a', '4', 'holiday', 'flights', 'inc', 'speak', 'to', 'a', 'live', 'operator', '2', 'claim', 'longdigit', 'p', 'min', 'number']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'winner u specially selected 2 receive å dollarsign number 4 holiday flight inc speak live operator 2 claim longdigit p min number'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_data(DATA_FILE)\n",
    "print(dataset.head())\n",
    "\n",
    "x = dataset.loc[116][1]\n",
    "print(x)\n",
    "preprocess(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = set(stopwords.words('english'))\n",
    "EMAIL_REGEX = r\"[\\w\\.-]+@[\\w\\.-]+\\.\\w+\"\n",
    "URL_REGEX = r\"https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+\"\n",
    "LONG_DIGIT_REGEX = r\"\\b\\d\\d\\d\\d\\d\\d\\d\\d+\\b\"\n",
    "MED_DIGIT_REGEX = r\"\\b0\\d\\d\\d+\\b\"\n",
    "SMALL_DIGIT_REGEX = r\"\\b0\\d+\\b\"\n",
    "NUMBER_REGEX = r\"\\b0|[1-9][0-9]+\\b\"\n",
    "TOKEN_PATTERN = r\"(?u)\\b\\w\\w+\\b|<\\w*>|\\?|\\\"|\\'\"\n",
    "LEMMATIZER = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t<longdigit>\tp fjdkjfkdj\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = '188888888888p fjdkjfkdj'\n",
    "\n",
    "LONG_DIGIT_REGEX = r\"\\b\\d\\d\\d\\d\\d\\d\\d\\d+\"\n",
    "long_digit_regex = re.compile(LONG_DIGIT_REGEX, re.IGNORECASE)\n",
    "text = long_digit_regex.sub(r'\\t<longdigit>\\t', test)\n",
    "\n",
    "print(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = text.replace('?', '\\tregexquestionmark\\t')\n",
    "    text = text.replace('$', '\\sregexdollarsign\\s')\n",
    "    text = text.replace('£', '\\sregexdollarsign\\s')\n",
    "    text = text.replace('!', '\\sregexexclamationmark\\s')\n",
    "\n",
    "    email_regex = re.compile(EMAIL_REGEX, re.IGNORECASE)\n",
    "    text = email_regex.sub(r'\\tregexemailaddress>\\t', text)\n",
    "\n",
    "    url_regex = re.compile(URL_REGEX, re.IGNORECASE)\n",
    "    text = url_regex.sub(r'\\t<url>\\t', text)\n",
    "\n",
    "    for letter in list(string.ascii_lowercase):\n",
    "        letter_regex = re.compile(r\"\\t[{}]\\t\".format(letter), re.IGNORECASE)\n",
    "        text = letter_regex.sub(r'\\t<{}>\\t'.format(letter), text)\n",
    "\n",
    "    long_digit_regex = re.compile(LONG_DIGIT_REGEX, re.IGNORECASE)\n",
    "    text = long_digit_regex.sub(r'\\t<longdigit>\\t', text)\n",
    "\n",
    "    med_digit_regex = re.compile(MED_DIGIT_REGEX, re.IGNORECASE)\n",
    "    text = med_digit_regex.sub(r'\\t<mediumdigit>\\t', text)\n",
    "\n",
    "    small_digit_regex = re.compile(SMALL_DIGIT_REGEX, re.IGNORECASE)\n",
    "    text = small_digit_regex.sub(r'\\t<smalldigit>\\t', text)\n",
    "\n",
    "    number_regex = re.compile(NUMBER_REGEX, re.IGNORECASE)\n",
    "    text = number_regex.sub(r'\\t<number>\\t', text)\n",
    "    \n",
    "    for punc in string.punctuation:\n",
    "        text = text.replace(punc, '\\t')\n",
    "    \n",
    "    text = text.split()\n",
    "    text = [LEMMATIZER.lemmatize(w) for w in text]\n",
    "    text = [w for w in text if w not in STOPWORDS]\n",
    "    text = ' '.join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "bad escape \\s at position 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mC:\\projects\\utilities\\python37\\lib\\sre_parse.py\u001b[0m in \u001b[0;36mparse_template\u001b[1;34m(source, pattern)\u001b[0m\n\u001b[0;32m   1020\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1021\u001b[1;33m                     \u001b[0mthis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mESCAPES\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mthis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1022\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '\\\\s'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-8c8c1b6dba4f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mngram_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlowercase\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken_pattern\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTOKEN_PATTERN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mDATA_SIZE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\projects\\vae-anomaly-detector\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1056\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1057\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m-> 1058\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m   1059\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1060\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\projects\\vae-anomaly-detector\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    968\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    969\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 970\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    971\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    972\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\projects\\vae-anomaly-detector\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m    350\u001b[0m                                                tokenize)\n\u001b[0;32m    351\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[1;32m--> 352\u001b[1;33m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[0;32m    353\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-47-4646e04da0c1>\u001b[0m in \u001b[0;36mpreprocess\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0murl_regex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mURL_REGEX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIGNORECASE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murl_regex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\s<url>\\s'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mletter\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mascii_lowercase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\projects\\utilities\\python37\\lib\\re.py\u001b[0m in \u001b[0;36m_subx\u001b[1;34m(pattern, template)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_subx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemplate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m     \u001b[1;31m# internal: Pattern.sub/subn implementation helper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 309\u001b[1;33m     \u001b[0mtemplate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_compile_repl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpattern\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    310\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtemplate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m         \u001b[1;31m# literal replacement\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\projects\\utilities\\python37\\lib\\re.py\u001b[0m in \u001b[0;36m_compile_repl\u001b[1;34m(repl, pattern)\u001b[0m\n\u001b[0;32m    298\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_compile_repl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpattern\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m     \u001b[1;31m# internal: compile replacement pattern\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0msre_parse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_template\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpattern\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    301\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_expand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemplate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\projects\\utilities\\python37\\lib\\sre_parse.py\u001b[0m in \u001b[0;36mparse_template\u001b[1;34m(source, pattern)\u001b[0m\n\u001b[0;32m   1022\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1023\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mASCIILETTERS\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1024\u001b[1;33m                         \u001b[1;32mraise\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'bad escape %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mthis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1025\u001b[0m                 \u001b[0mlappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1026\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: bad escape \\s at position 0"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(dataset.v1)\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=2, ngram_range=(1, 1), lowercase=True, preprocessor=preprocess, token_pattern=TOKEN_PATTERN)\n",
    "inputs = vectorizer.fit_transform(dataset.v2).toarray()\n",
    "\n",
    "DATA_SIZE = len(dataset)\n",
    "VOCAB_SIZE = len(vectorizer.vocabulary_)\n",
    "print(VOCAB_SIZE)\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = int(0.7 * DATA_SIZE)\n",
    "VALID_SIZE = int(0.15 * DATA_SIZE)\n",
    "TEST_SIZE = DATA_SIZE - VALID_SIZE - TRAIN_SIZE\n",
    "\n",
    "END_VALID = TRAIN_SIZE + VALID_SIZE\n",
    "\n",
    "train_inputs = torch.from_numpy(inputs[:TRAIN_SIZE]).type(torch.float32)\n",
    "\n",
    "valid_inputs = torch.from_numpy(inputs[TRAIN_SIZE:END_VALID]).type(torch.float32)\n",
    "valid_labels = torch.from_numpy(labels[TRAIN_SIZE:END_VALID]).type(torch.float32)\n",
    "\n",
    "test_inputs = torch.from_numpy(inputs[END_VALID:]).type(torch.float32)\n",
    "test_labels = torch.from_numpy(labels[END_VALID:]).type(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier_initialization(layer):\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        bound = 1 / math.sqrt(layer.in_features)\n",
    "        layer.weight.data.uniform_(-bound, bound)\n",
    "        layer.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"Probabilistic Encoder. Returns the mean and the variance of z ~ q(z|x). The prior\n",
    "    of z is assume to be normal(0, I).\n",
    "\n",
    "    :params input_dim: number of features\n",
    "\n",
    "    :return: mean and variance of the latent variable\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.encoder_network = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.BatchNorm1d(128),\n",
    "\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.BatchNorm1d(64)\n",
    "        )\n",
    "        self.read_mu = nn.Linear(64, latent_dim)\n",
    "        self.read_logvar = nn.Linear(64, latent_dim)\n",
    "        self.apply(xavier_initialization)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        hidden_state = self.encoder_network(inputs)\n",
    "        mean = self.read_mu(hidden_state)\n",
    "        logvar = self.read_logvar(hidden_state)\n",
    "        return mean, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.BatchNorm1d(64),\n",
    "\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.BatchNorm1d(128),\n",
    "\n",
    "            nn.Linear(128, input_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.read_alpha = nn.Sequential(\n",
    "            nn.Linear(latent_dim, input_dim),\n",
    "            nn.ReLU6()\n",
    "        )\n",
    "        self.apply(xavier_initialization)\n",
    "\n",
    "    def forward(self, z):\n",
    "        alpha = 0.5 * self.read_alpha(z)\n",
    "        loglambda = alpha * self.decoder(z) \n",
    "        return loglambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    \"\"\"\n",
    "    VAE, x --> mu, log_sigma_sq --> N(mu, log_sigma_sq) --> z --> x\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, latent_dim, device=torch.device('cpu')):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = Encoder(input_dim, latent_dim).to(device)\n",
    "        self.decoder = Decoder(input_dim, latent_dim).to(device)\n",
    "\n",
    "    def parameters(self):\n",
    "        return chain(self.encoder.parameters(), self.decoder.parameters())\n",
    "\n",
    "    def sample_z(self, mu, logvar):\n",
    "        epsilon = torch.randn_like(logvar)\n",
    "        sigma = torch.exp(logvar / 2)\n",
    "        return mu + sigma * epsilon\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        mu, logvar = self.encoder(inputs)\n",
    "        latent = self.sample_z(mu, logvar)\n",
    "        theta = self.decoder(latent)\n",
    "        return theta, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence(mu, logvar):\n",
    "    \"\"\"Compute Gaussian KL-divergence\"\"\"\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    return -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "def loss_fn(loglambda, inputs, *latent_params):\n",
    "    \"\"\"Cross entropy + KL divergence losses summed over all elements and batch\"\"\"\n",
    "    cross_entropy = F.poisson_nll_loss(loglambda, target=inputs, reduction='sum')\n",
    "    kl_div = kl_divergence(*latent_params)\n",
    "    \n",
    "    return cross_entropy + kl_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_inputs, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ITER = 100\n",
    "LATENT_DIM = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE(input_dim=VOCAB_SIZE, latent_dim=LATENT_DIM, device=DEVICE).to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "#train_x = train_inputs.to(DEVICE)\n",
    "#valid_x = valid_inputs.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf291da23cf94ed4a2fd77ae82ce06bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, layout=Layout(flex='2')), HTML(value='')), layout=Layout(display='inline-f…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 150.00 MiB (GPU 0; 2.00 GiB total capacity; 1.04 GiB already allocated; 19.39 MiB free; 162.19 MiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-a4a58e8c4d45>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMAX_ITER\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m     \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-a4a58e8c4d45>\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(train_x)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mloglambda\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogvar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloglambda\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-526847fe4cfd>\u001b[0m in \u001b[0;36mloss_fn\u001b[1;34m(loglambda, inputs, *latent_params)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloglambda\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mlatent_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;34m\"\"\"Cross entropy + KL divergence losses summed over all elements and batch\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mcross_entropy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpoisson_nll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloglambda\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sum'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mkl_div\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkl_divergence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlatent_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\projects\\vae-anomaly-detector\\venv\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mpoisson_nll_loss\u001b[1;34m(input, target, log_input, full, size_average, eps, reduce, reduction)\u001b[0m\n\u001b[0;32m   1887\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" is not valid\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1888\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1889\u001b[1;33m     \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpoisson_nll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfull\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1890\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1891\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 150.00 MiB (GPU 0; 2.00 GiB total capacity; 1.04 GiB already allocated; 19.39 MiB free; 162.19 MiB cached)"
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('Inf')\n",
    "pbar = tqdm(range(MAX_ITER), ncols=1000, unit=' epoch')\n",
    "\n",
    "def train_step(train_x):\n",
    "    model.train()\n",
    "    train_x = train_x.to(DEVICE)\n",
    "    optimizer.zero_grad()\n",
    "    loglambda, mu, logvar = model(train_x)\n",
    "    loss = loss_fn(loglambda, train_x, mu, logvar)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "def eval_step(train_x, valid_x):\n",
    "    train_x = train_x.to(DEVICE)\n",
    "    valid_x = valid_x.to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        train_output = model(train_x)\n",
    "        valid_output = model(valid_x)\n",
    "        \n",
    "        train_kld = kl_divergence(*train_output[1:]).cpu() / train_x.shape[0]\n",
    "        valid_kld = kl_divergence(*valid_output[1:]).cpu() / valid_x.shape[0]\n",
    "        \n",
    "        train_pce = F.poisson_nll_loss(train_output[0], target=train_x, reduction='sum').cpu() / train_x.shape[0]\n",
    "        valid_pce = F.poisson_nll_loss(valid_output[0], target=valid_x, reduction='sum').cpu() / valid_x.shape[0]\n",
    "        \n",
    "        valid_loss = valid_pce + valid_kld\n",
    "        \n",
    "        template = 'Train PCE: {:.2f} | Train KLD: {:.2f} | Valid PCE: {:.2f} | Valid KLD {:.2f}'\n",
    "        pbar.set_description(template.format(train_pce, train_kld, valid_pce, valid_kld))\n",
    "        pbar.update()\n",
    "    return valid_loss\n",
    "\n",
    "for epoch in range(MAX_ITER):\n",
    "    \n",
    "    train_step(train_inputs)\n",
    "    valid_loss = eval_step(train_inputs, valid_inputs)\n",
    "    \n",
    "        \n",
    "    # early stopping\n",
    "    if valid_loss > best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        # best_weights = deepcopy(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best_valid_loss = float('Inf')\n",
    "pbar = tqdm(range(MAX_ITER), ncols=1000, unit=' epoch')\n",
    "\n",
    "def train_step(loader):\n",
    "    model.train()\n",
    "    for batch_input in loader:\n",
    "        optimizer.zero_grad()\n",
    "        batch_input = batch_input.to(DEVICE)\n",
    "        loglambda, mu, logvar = model(batch_input)\n",
    "        loss = loss_fn(loglambda, batch_input, mu, logvar)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "def eval_step(train_x, valid_x):\n",
    "    train_x = train_x.to(DEVICE)\n",
    "    valid_x = valid_x.to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        train_output = model(train_x)\n",
    "        valid_output = model(valid_x)\n",
    "        \n",
    "        train_kld = kl_divergence(*train_output[1:]).cpu() / train_x.shape[0]\n",
    "        valid_kld = kl_divergence(*valid_output[1:]).cpu() / valid_x.shape[0]\n",
    "        \n",
    "        train_pce = F.poisson_nll_loss(train_output[0], target=train_x, reduction='sum').cpu() / train_x.shape[0]\n",
    "        valid_pce = F.poisson_nll_loss(valid_output[0], target=valid_x, reduction='sum').cpu() / valid_x.shape[0]\n",
    "        \n",
    "        valid_loss = valid_pce + valid_kld\n",
    "        \n",
    "        template = 'Train PCE: {:.2f} | Train KLD: {:.2f} | Valid PCE: {:.2f} | Valid KLD {:.2f}'\n",
    "        pbar.set_description(template.format(train_pce, train_kld, valid_pce, valid_kld))\n",
    "        pbar.update()\n",
    "    return valid_loss\n",
    "\n",
    "for epoch in range(MAX_ITER):\n",
    "    \n",
    "    train_step(train_loader)\n",
    "    valid_loss = eval_step(train_inputs, valid_inputs)\n",
    "    \n",
    "        \n",
    "    # early stopping\n",
    "    if valid_loss > best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        # best_weights = deepcopy(model.state_dict())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
